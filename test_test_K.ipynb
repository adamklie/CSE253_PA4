{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "from word_holder import build_word_holder\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import *\n",
    "import torch\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "from TheRealNapster import *\n",
    "from word_holder import word_holder\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize_batch(image_batch):\n",
    "    inv_batch = torch.empty(image_batch.size())\n",
    "    for i, image in enumerate(image_batch):\n",
    "        inv_normalize = transforms.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], \n",
    "                                             std=[1/0.229, 1/0.224, 1/0.225])\n",
    "        orig_image = inv_normalize(image)\n",
    "        inv_batch[i] = orig_image\n",
    "    return inv_batch\n",
    "  \n",
    "    \n",
    "def get_words(captions, vocab):\n",
    "    sampled_caption = []\n",
    "    for cap in captions:\n",
    "        resampleThis = []\n",
    "        for word_id in cap:\n",
    "            word = vocab.idx_to_word[word_id]\n",
    "            if word == '<start>':\n",
    "                continue\n",
    "            if word == '<end>':\n",
    "                break\n",
    "            resampleThis.append(word)\n",
    "        sampled_caption.append(\" \".join(resampleThis))\n",
    "#     for sample in sampled_caption:\n",
    "#         print(sample)\n",
    "    return sampled_caption\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, run_id):\n",
    "    \n",
    "    # Create SummaryWriter object for tracking using tensorboard\n",
    "    writer = SummaryWriter('{0}/{1}/{2}'.format(args.tensorboard_path, run_id, args.run_id))\n",
    "    \n",
    "    test_transforms = transforms.Compose([\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load vocab built from build_vocab.py\n",
    "    with open('data/vocab.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "\n",
    "    # Load IDs that will be used for training\n",
    "    with open('data/test_filtered_ids.pkl', 'rb') as f:\n",
    "        IDs = pickle.load(f)\n",
    "\n",
    "    test_cocoloader = get_loader(root=args.image_path, json=args.annotation_path, \n",
    "                                  ids=IDs, vocab=vocab, transform=test_transforms,\n",
    "                                  batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    \n",
    "    # Build models\n",
    "    encoder = Enigma(args.embed_dim).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "    decoder = Christopher(len(vocab), args.embed_dim, args.units_per_layer, args.num_layers)\n",
    "    \n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load('models/Enigma_5-500_lr-0.001_nl-1_hls-512_es-256_bs-128_t-lstm_pte-False.ckpt'))\n",
    "    decoder.load_state_dict(torch.load('models/Christopher_5-500_lr-0.001_nl-1_hls-512_es-256_bs-128_t-lstm_pte-False.ckpt'))\n",
    "\n",
    "    for i, (image_batch, caption_batch, length_batch) in enumerate(test_dataloader):\n",
    "        image_batch = image_batch.to(device)\n",
    "        caption_batch = caption_batch.to(device)\n",
    "        target_batch = pack_padded_sequence(caption_batch, length_batch, batch_first=True)[0]\n",
    "       \n",
    "        # Run through model\n",
    "        encoder_features = encoder(image_batch)\n",
    "        output = decoder(encoder_features, caption_batch, length_batch)\n",
    "        \n",
    "        # For the first batch, display how we are doing on four images\n",
    "        if i == 0:\n",
    "            un_batch = unnormalize_batch(image_batch[0:4])\n",
    "            writer.add_images('Test Batch Sample', un_batch, 1)\n",
    "            caption_ids = decoder.ItsGameTime(encoder_features)\n",
    "            caption_ids = caption_ids[0:4].cpu().numpy() \n",
    "            caption = get_words(caption_ids, vocab)\n",
    "            real_caption = get_words(vocab=vocab, captions=caption_batch[0:4].cpu().numpy())\n",
    "            writer.add_text('Test Image 1 Predicted Caption', caption[0], 1)\n",
    "            writer.add_text('Test Image 1 Actual Caption', real_caption[0], 1)\n",
    "            writer.add_text('Test Image 2 Predicted Caption', caption[1], 2)\n",
    "            writer.add_text('Test Image 2 Actual Caption', real_caption[1], 2)\n",
    "            writer.add_text('Test Image 3 Predicted Caption', caption[2], 3)\n",
    "            writer.add_text('Test Image 3 Actual Caption', real_caption[2], 3)            \n",
    "            writer.add_text('Test Image 4 Predicted Caption', caption[3], 4)\n",
    "            writer.add_text('Test Image 4 Actual Caption', real_caption[3], 4)  \n",
    "            for j, (rcap, gcap) in enumerate(zip(real_caption, caption)):\n",
    "                print(j + 1)\n",
    "                print(rcap)\n",
    "                print(gcap)\n",
    "            writer.flush()\n",
    "            break\n",
    "            \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    # File structure arguments\n",
    "    path_args = parser.add_argument_group('Input/output options:')\n",
    "    path_args.add_argument('--run_id', type=str, default='test', help='tensorboard subdirectory')\n",
    "    path_args.add_argument('--tensorboard_path', type=str, default='tensorboard_kk', help='Directory for Tensorboard output')\n",
    "    path_args.add_argument('--model_path', type=str, default='models', help='Directory for saved model checkpoints')\n",
    "    path_args.add_argument('--image_path', type=str, default='data/images/test_resized', help='Directory with test images')\n",
    "    path_args.add_argument('--annotation_path', type=str, default='data/annotations/captions_val2014.json', help='Directory with training annotations')\n",
    "    \n",
    "    # Model structure arguments\n",
    "    model_args = parser.add_argument_group('Model structure options:')\n",
    "    model_args.add_argument('--embed_dim', type=int, default=256, help='Dimensions of word embedding to use')\n",
    "    model_args.add_argument('--num_layers', type=int, default=1, help='Number of hidden layers in model')\n",
    "    model_args.add_argument('--units_per_layer', type=int, default=512, help='Number of hidden units in each hidden layer')\n",
    "    model_args.add_argument('--unit_type', type=str, default='lstm', help='Defines unit, either lstm or rnn')\n",
    "    model_args.add_argument('--pretrained_embedding', type=bool, default=False, help='Boolean flag for pretrained embeddings')\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = parser.add_argument_group('Training options:')\n",
    "    training_args.add_argument('--batch_size', type=int, default=128, help='Batch size for mini-batch gradient descent')\n",
    "    training_args.add_argument('--num_workers', type=int, default=2, help='Number of workers for dataloading')\n",
    "    training_args.add_argument('--validation_split', type=float, default=0.2, help='Validation split percentage for training')\n",
    "    training_args.add_argument('--num_epochs', type=int, default=5, help='Number of epochs to train on')\n",
    "    training_args.add_argument('--learning_rate', type=float, default=0.001, help='Set learning rate for training')\n",
    "    \n",
    "    # Logging arguments\n",
    "    log_args = parser.add_argument_group('Logging options:')\n",
    "    log_args.add_argument('--log_step', type=int, default=10, help='Number of batches between printing status')\n",
    "    log_args.add_argument('--save_step', type=int, default=250, help='Number of batches between saving models')\n",
    "                           \n",
    "    args = parser.parse_args([])\n",
    "    \n",
    "    run_id = 'lr-{0}_nl-{1}_hls-{2}_es-{3}_bs-{4}_t-{5}_pte-{6}_resize'.format(args.learning_rate, args.num_layers,\n",
    "                                                                        args.units_per_layer, args.embed_dim,\n",
    "                                                                        args.batch_size, args.unit_type,\n",
    "                                                                        args.pretrained_embedding)\n",
    "                           \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "#     main(args, run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.34s)\n",
      "creating index...\n",
      "index created!\n",
      "Not pre-initializing...\n",
      "Using lstm model...\n",
      "0\n",
      "['an elephant playing with water at a watering hole with his trunk', \"an elephant's front legs are in the water and back legs are out of the water.\", 'an elephant is going to the river to drink water', 'an elephant standing with its front feet in the water.', 'an elephant stands with its front feet in shallow water. ']\n",
      "['a group of elephants standing in a field with trees in the background .']\n",
      "0.8671924973587646\n",
      "0.3243406530946741\n",
      "100\n",
      "['a white demon hovering next to a red stop sign under a one way street sign.', 'a statue appears to look toward a stop sign.', 'odd, almost humorous human figure sticking out from a yellow pole, near intersection with stop and one way signs, night time, with car streaking by and very strong light in the background.', 'altered stop sign sitting in a growth of vegetation. ', 'a strange sculpture being pointed at by a one way sign']\n",
      "['a stop sign with a street sign on it .']\n",
      "0.8539396656235351\n",
      "0.5855586278561383\n",
      "200\n",
      "['some luggage and a back back sitting on teh ground next to a lap top', 'a suitcase and a laptop sit on a bedroom floor', 'two pieces of luggage sitting near a dresser and laptop.', 'a bedroom with a laptop, suitcase and backpack on the floor.', 'corner of a bedroom with a laptop and luggage on a floor.']\n",
      "['a laptop computer sitting on a desk next to a laptop .']\n",
      "0.9101048640623259\n",
      "0.5101625881595045\n",
      "300\n",
      "['luggage sitting on a bed in a hotel room.', 'luggage sitting on a bed in a hotel room. ', 'a set of luggage on a hotel room bed.', 'a group of suitcases sitting on a bed', 'a hotel room bed has two luggage cases sitting on it.']\n",
      "['a bed with a white bedspread and a red bedspread .']\n",
      "0.6215645921656041\n",
      "0.060112629803249905\n",
      "400\n",
      "['a couple bags of luggage that are on the ground.', 'four different eras of luggage are tossed along a rail.', 'four different types of carrying cases in various colors.', 'four pieces of luggage sit on the ground. ', 'a pile of luggage sitting on the ground at an airport.']\n",
      "['a pile of luggage sitting on top of a wooden floor .']\n",
      "0.8697428764437133\n",
      "0.5302296997511825\n",
      "500\n",
      "['there is a boy playing a game of tennis.', 'young boy on court playing tennis wearing red shirt.', 'a young boy is in a tennis court and has his hand with the racket up and ready to receive a tennis ball.', 'a boy is holding a tennis racket outside', 'a tennis player with a tennis racket ']\n",
      "['a man holding a tennis racquet on a tennis court .']\n",
      "0.9415736503692766\n",
      "0.6950391687484891\n",
      "600\n",
      "['two people flying a kite on a cloudy day.', 'people in a field flying a kite with large clouds in the sky. ', 'two people flying a kite in a wide open space ', 'a couple of people flying kites on top of a grass covered field.', 'the two people are flying their kites on this cloudy day.']\n",
      "['a person flying a kite in the sky .']\n",
      "0.8424604416167714\n",
      "0.5791915536115303\n",
      "700\n",
      "['a ham sandwich with lettuce in a paper bag', 'a yummy sandwich on ciabatta bread with meat and vegetables.', 'a sandwich containing meat and cheese inside of a bag.', 'a hero sandwich with prosciutto and other ingredients inside a sandwich bag', 'a sandwich with prosciutto and artisan bread sits inside a bag.']\n",
      "['a sandwich with a pickle and a pickle on a plate .']\n",
      "0.8308047117479722\n",
      "0.3731746932201293\n",
      "800\n",
      "['a group of men standing around a sidewalk together.', 'men staning in the street and holding cell phones', 'all of these men are using their cell phones.', 'a group of people on a sidewalk with their mobile phones.', 'men are standing on the street behind a barrier, using their cell phones.']\n",
      "['a man and woman are standing in a room .']\n",
      "0.7942472123261359\n",
      "0.429322817473587\n",
      "900\n",
      "['a man power sliding on a long board ', 'a young man sitting  on his skateboard touching the ground', 'a man riding a skateboard down a street.', 'young man with skateboard appearing like he just fell down.', 'a man doing a trick on a skateboard in the middle of the street.']\n",
      "['a man on a skateboard is doing a trick .']\n",
      "1.0\n",
      "0.8108108108108109\n",
      "1000\n",
      "['an open laptop computer sitting on top of a wooden table.', 'a laptop computer sitting open on a desk amidst other things', 'the papers lie haphazardly beside the small laptop.', 'small silver laptop sitting on top of a wooden computer desk. ', 'a desk covered with paperwork and a laptop ']\n",
      "['a desk with a laptop , computer , and a monitor .']\n",
      "0.9208215457228051\n",
      "0.521742087655151\n",
      "1100\n",
      "['a batter, catcher and umpire in a baseball game.', 'a baseball player holding a baseball bat on a field', 'a game of baseball being played in front of a large crowd at a stadium.', 'a gentleman in white is ready to kick a ball holding a bat', 'an image of a baseball game being played in progress']\n",
      "['a baseball player swinging a bat at a ball']\n",
      "0.8668778997501817\n",
      "0.6223738767437202\n",
      "1200\n",
      "['an advertisement from a christmas wonderland store with several ge products.', \"a page in a general electric's catalog advertising christmas ideas.\", 'electronics advertised on a paper for christmas. ', 'an ad for general electric with clocks and appliances on it', 'a various items such clocks and kitchen appliances are advertized. ']\n",
      "['a bunch of different types of different types of doughnuts on a table .']\n",
      "0.7887323943661971\n",
      "0.08823529411764706\n",
      "1300\n",
      "['various antique chain clocks kept altogether on a table.', 'a lot of small clocks on different times.', 'a large array of pocket watches are on a table.', 'a table with several pocket watches on it.', 'a collection of antique pocket watches in various condition']\n",
      "['a bunch of different types of different types of <err> .']\n",
      "0.7857142857142857\n",
      "0.20754716981132076\n",
      "1400\n",
      "['empty bar or lounge before or after business hours.', 'a restaurant that has many tables set up.', \"an empty bar looks empty with all of it's clean tables. \", 'dining tables and chairs in a restaurant with dim lighting.', 'a dinning hall with dark wooden furniture and circular light fixtures. ']\n",
      "['a kitchen with a table and chairs and a table']\n",
      "0.9777777777777777\n",
      "0.42857142857142855\n",
      "1500\n",
      "['thee family is standing around the island in the kitchen.', 'a group of people that are in a kitchen.', 'three people people in a kitchen preparing food and washing hands.', 'a couple of women and one man preparing food in a kitchen.', 'two women and a man talking in a kitchen']\n",
      "['a man and woman sitting at a table with a cake .']\n",
      "0.8958333333333334\n",
      "0.24444444444444446\n",
      "1600\n",
      "['professional athlete on clay court during tennis match.', \"the tennis player's hair stands on end while balancing on one leg. \", 'a man who is lifting one leg up on a tennis court.', 'a man on a clay tennis ccourt serving the ball.', 'a tennis player stands on one leg, and makes a face.']\n",
      "['a man holding a tennis racquet on a tennis court .']\n",
      "0.98\n",
      "0.425531914893617\n",
      "1700\n",
      "['a stop sign with a sticker that says \"worrying\" fastened to it.', 'a stop sign with a sticker attached to it', 'stop sign with sarcastic comment placed underneath it', 'a red stop sign sitting under lush green trees.', \"a stop sign with the word 'worrying' added to it. \"]\n",
      "['a stop sign with a street sign on it .']\n",
      "0.9240885594051769\n",
      "0.5016480751056674\n",
      "1800\n",
      "['people in navy uniforms and one person talking on a walkie- talkie.', 'group of sailors in command center with one talking on walkie talkie. ', 'many members of the navy work while on a ship.', 'a crew of sea men commanding a boat from the command center.', 'seamen inside a navy vessel communicate over the radio.']\n",
      "['a man with a beard and a woman in a suit']\n",
      "0.817672577603805\n",
      "0.25588615515339563\n",
      "1900\n",
      "['a horse standing in the grass in a fenced in area.', 'a horse grazing the field with a fence surrounding it. ', 'horse in a large corral eating grass and trees in the back. ', 'a green pasture with a single brown horse.', 'brown horse in green field with trees and fence']\n",
      "['a horse grazing in a field with a green grass .']\n",
      "0.9787234042553191\n",
      "0.8863636363636364\n",
      "2000\n",
      "['a lady is standing in pastel colored bathroom in front of the bathtub and there are christmas lights hanging up outside of the doorway. ', 'there is a doll standing in the middle of a toy bathroom.', 'a person and a toilet standing in a room.', 'a lady dressed in khakis standing in a bathroom next to the sink. ', 'lady standing in a retro pink and turquoise bathroom.']\n",
      "['a man in a suit and tie standing on a sidewalk .']\n",
      "0.9010751057212905\n",
      "0.30035836857376347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2100\n",
      "['a couple of brown elephants standing next to each other.', 'two big elephants standing near a zoo keeper', 'a man is standing between two elephants in their pen.', 'a man standing between two large elephants outside', 'a man is standing between two elephants at the zoo.']\n",
      "['a cow standing in the middle of a dirt road .']\n",
      "0.9111111111111111\n",
      "0.38095238095238093\n",
      "2200\n",
      "['a flock of birds flying in an overcast sky', 'several birds are flying in a group across the sky.', 'the flock of birds are traveling closely together in the sky.', 'a large group of birds can be seen flying in the sky.', 'looking up at a flock of small birds in the sky']\n",
      "['a flock of birds flying over a body of water .']\n",
      "0.8934088995801787\n",
      "0.5233812933310238\n",
      "2300\n",
      "['a set of five train tracks in front of a graffiti covered wall.', 'a train car that has some graffiti on it.', 'some train cars with graffiti painted on them', 'spray painted boxcars are parked in a mostly empty train yard.', 'a train car with some graffit all over the side of it ']\n",
      "['a black and white photo of a street sign .']\n",
      "1.0\n",
      "0.07692307692307693\n",
      "2400\n",
      "['two bikes parked next to each other on a bike rack.', 'a white and a black bicycle parked in a concrete bicycle parking station, next to a grass yard area.', \"a man and woman's bike parked in a bike rack.\", 'two bicycles parked in a bike rack near the grass.', 'a couple of bikes that are on the street.']\n",
      "['a motorcycle parked on the side of a road .']\n",
      "0.9767441860465116\n",
      "0.45\n",
      "2500\n",
      "['baseball players in the middle of a game with a small crowd.', 'a baseball player taking a swing at a ball', 'a person in a baseball uniform holding a bat. ', 'a baseball player hitting a ball in a game.', 'a couple of kids that are playing baseball']\n",
      "['a baseball player is getting ready to throw a ball .']\n",
      "0.9230769230769231\n",
      "0.46938775510204084\n",
      "2600\n",
      "['a bunch of people in turbans at a parade.', 'a group of bikers that are on a road', 'many men wearing vests and riding motorcycles in a parade.', 'a group of motorcycle riders in vests and orange turbans on a neighborhood street.', 'a group of men on the street wearing orange turbans.']\n",
      "['a group of motorcycles driving down a street .']\n",
      "1.0\n",
      "0.627906976744186\n",
      "2700\n",
      "['adult giraffe with young standing near wall and tree in enclosed area.', 'a tall giraffe bending down to snuggle a baby giraffe.', 'a mother giraffe and her baby are touching nose to nose.    ', \"there's a mamma giraffe kissing her baby giraffe \", 'a large giraffe and a small giraffe behind a fence']\n",
      "['a giraffe standing in a field with a tree']\n",
      "0.8026676277896609\n",
      "0.45466896547822244\n",
      "2800\n",
      "['a person near a bike and a car on a street.', 'a parked motorcycle with a cat and a person leaning on top of it.', 'a woman is next to a scooter and cat.', 'a woman is looking at a cat on a bike', 'a cat sitting on a motorcycle that is parked in a driveway.']\n",
      "['a man on a motorcycle with a man on his back .']\n",
      "0.9347826086956522\n",
      "0.5116279069767442\n",
      "2900\n",
      "['skier poses in front of a scenic mountain view.', 'a young man in his skiing gear is posing for the camera.', 'a man on a mountain smiles while holding ski equipment.', 'a snow skier wearing a red ski jacket goggles and a helmet ', 'a young skier holding his skis posing for a picture .']\n",
      "['a man on skis standing on a snowy surface']\n",
      "0.863862675318009\n",
      "0.40919810936116213\n"
     ]
    }
   ],
   "source": [
    "# Create SummaryWriter object for tracking using tensorboard\n",
    "writer = SummaryWriter('{0}/{1}/{2}'.format(args.tensorboard_path, run_id, args.run_id))\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.RandomCrop(224),\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Load vocab built from build_vocab.py\n",
    "with open('data/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# test image IDs\n",
    "test_img_id_path = 'TestImageIds.csv'\n",
    "test_cocoloader = CocoDataset2(root=args.image_path, json=args.annotation_path,\n",
    "                               img_id_path=test_img_id_path, vocab=vocab, transform=test_transforms)\n",
    "\n",
    "# Build models\n",
    "encoder = Enigma(args.embed_dim).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "decoder = Christopher(len(vocab), args.embed_dim, args.units_per_layer, args.num_layers)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load('models/Enigma_5-500_lr-0.001_nl-1_hls-512_es-256_bs-128_t-lstm_pte-False.ckpt'))\n",
    "decoder.load_state_dict(torch.load('models/Christopher_5-500_lr-0.001_nl-1_hls-512_es-256_bs-128_t-lstm_pte-False.ckpt'))\n",
    "\n",
    "# Initialization for BLEU scores\n",
    "score1 = 0\n",
    "score4 = 0\n",
    "smoother = SmoothingFunction()\n",
    "\n",
    "for i in range(len(test_cocoloader)):\n",
    "    image, caption_list = test_cocoloader[i]\n",
    "    image_batch = image.unsqueeze(0)\n",
    "    image_batch = image_batch.to(device)\n",
    "\n",
    "    # Run through model\n",
    "    encoder_features = encoder(image_batch)\n",
    "    output = decoder.ItsGameTime(encoder_features) #, caption_batch, length_batch)\n",
    "    gen_caption = get_words(output.cpu().numpy(), vocab)\n",
    "    \n",
    "    score1 += sentence_bleu(caption_list, gen_caption[0], weights=(1, 0, 0, 0), smoothing_function=smoother.method1)\n",
    "    score4 += sentence_bleu(caption_list, gen_caption[0], weights=(0, 0, 0, 1), smoothing_function=smoother.method1)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "        print(caption_list)\n",
    "        print(gen_caption)\n",
    "        print(sentence_bleu(caption_list, gen_caption[0], weights=(1, 0, 0, 0), smoothing_function=smoother.method1))\n",
    "        print(sentence_bleu(caption_list, gen_caption[0], weights=(0, 0, 0, 1), smoothing_function=smoother.method1))\n",
    "    # For the first batch, display how we are doing on four images\n",
    "#     if i == 0:\n",
    "#         un_batch = unnormalize_batch(image_batch[0:4])\n",
    "#         writer.add_images('Test Batch Sample', un_batch, 1)\n",
    "#         caption_ids = decoder.ItsGameTime(encoder_features)\n",
    "#         caption_ids = caption_ids[0:4].cpu().numpy() \n",
    "#         caption = get_words(caption_ids, vocab)\n",
    "#         real_caption = get_words(vocab=vocab, captions=caption_batch[0:4].cpu().numpy())\n",
    "#         writer.add_text('Test Image 1 Predicted Caption', caption[0], 1)\n",
    "#         writer.add_text('Test Image 1 Actual Caption', real_caption[0], 1)\n",
    "#         writer.add_text('Test Image 2 Predicted Caption', caption[1], 2)\n",
    "#         writer.add_text('Test Image 2 Actual Caption', real_caption[1], 2)\n",
    "#         writer.add_text('Test Image 3 Predicted Caption', caption[2], 3)\n",
    "#         writer.add_text('Test Image 3 Actual Caption', real_caption[2], 3)            \n",
    "#         writer.add_text('Test Image 4 Predicted Caption', caption[3], 4)\n",
    "#         writer.add_text('Test Image 4 Actual Caption', real_caption[3], 4)  \n",
    "#         for j, (rcap, gcap) in enumerate(zip(real_caption, caption)):\n",
    "#             print(j + 1)\n",
    "#             print(rcap)\n",
    "#             print(gcap)\n",
    "#         writer.flush()\n",
    "#         break\n",
    "bleu1 = 100*score1/len(test_cocoloader)\n",
    "bleu4 = 100*score4/len(test_cocoloader)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 57])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a cake with a plate of cake on it']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words(output.cpu().numpy(), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = ['luggage sitting on a bed in a hotel room.', 'luggage sitting on a bed in a hotel room. ', 'a set of luggage on a hotel room bed.', 'a group of suitcases sitting on a bed', 'a hotel room bed has two luggage cases sitting on it.']\n",
    "candidate = '<start> a bed with a white bedspread and a red bedspread . <end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.578125"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smoother.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start>',\n",
       " 'a',\n",
       " 'bed',\n",
       " 'with',\n",
       " 'a',\n",
       " 'white',\n",
       " 'bedspread',\n",
       " 'and',\n",
       " 'a',\n",
       " 'red',\n",
       " 'bedspread',\n",
       " '.',\n",
       " '<end>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
